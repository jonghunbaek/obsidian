# 1. 대용량 메모리 기기 대상 배포 전략
## 1.1 배경
- 하루 페이지 뷰가 15만에 육박
- 성능 개선을 위해 서버의 Scale Up을 수행
	- 32비트 OS -> 64비트 OS
	- 힙 사이즈 1.5GB -> 12GB(전체 메모리는 16GB로)
- 서버의 하드웨어 성능 향상 대비 실제 애플리케이션의 성능은 비슷하거나 오히려 떨어지는 경우가 발생

## 1.2 원인
- 문제의 핵심 원인은 GC
	- JDK 5 핫스파의 기본 설정인 패러렐 컬렉터를 사용(패러렐 스캐빈지+패러렐 올드)
	- 일시 정지 시간보단 처리량에 중점을 둔 컬렉터
	- 12GB의 힙 메모리를 전체 GC하며 최대 14초까지 일시 정지하게 됨
	- 웹 페이지를 직렬화하는 과정에서 메모리에 수많은 거대 객체가 쌓이며 대부분 구세대에 생성됨
	- **즉, 힙 메모리를 너무 크게 잡아 회수/재활용에 너무 오랜 시간이 걸림**

## 1.3 해결
현재 상황(대용량 메모리를 갖춘 하드웨어)에서 배포 방식에 따라 두 가지 방안을 사용할 수 있음
- **VM 인스턴스 하나가 거대한 자바 힙 메모리를 관리**
- **VM 여러 개를 동시에 띄워 놓는 논리적 클러스터 구성**

기존엔 관리자가 첫 번째 방식을 사용한다. 만약 이 방식으로 문제를 해결하려면 셰넌도어 또는 ZGC와 같이 **지연 시간 통제를 목표로하는 GC를 이용**해야 한다. (JDK 5기준엔 해당 GC들이 존재하지 않았음)
패러렐 컬렉터를 사용해 해결하려면 **전체 GC 빈도를 최대한 낮추는 것이 중요**하다. 이 때,  전체 GC 빈도를 제어하려면 구세대가 안정되어야 한다. **즉, 오래 생존하는 객체가 적어야 한다는 뜻이다.**(객체의 생존 범위가 요청/페이지 범위를 넘어서면 안됨)

이외에도 첫 번째 방식으로 배포를 하게 되면 아래와 같은 사항들을 추가적으로 검토해야 한다.
- 힙 메모리의 거대 블록 회수로 인한 긴 시간의 일시 정지
- 64비트 VM의 성능은 일반적으로 동일 버전의 32비트 VM보다 조금씩 느림
- 안정적인 애플리케이션
	- 대규모 단일 애플리케이션에선 힙 덤프 스냅숏을 생성하기도 힘들고 생성해도 분석이 어려움
- 32비트 VM보다 64비트 VM에서 더 많은 메모리를 사용

이러한 요소들을 고려했을 때, 저자는 두 번째 방식의 배포가 적합하다고 판단했다.
물론 이 방식에도 아래와 같은 단점들이 존재한다.
- 클러스터 내 노드들의 전역 자원에 대한 경합
- 커넥션 풀과 같은 자원 풀을 효율적으로 활용하기 어려움
	- 중앙화된 JNDI로 해결할 수 있지만 복잡도/성능 비용 증가
- 클러스터 노드로 32비트 VM을 쓴다면 노드별 메모리는 하드웨어 성능과 관계 없이 32비트 메모리 최대 사용량(4GB)을 넘을 수 없다. 
- 로컬 캐시를 많이 사용한다면 상당량의 메모리가 낭비됨
	- 글로벌 캐시를 사용해 해결

최종적으로 32비트 VM 5대로 논리 클러스터를 구축하고, 각 메모리를 2GB씩 할당하여 총 10GB의 메모리를 활용하도록 했다. 또한 사용자의 응답 속도를 고려해 패러렐 컬렉터에서 CMS로 변경했다. 

# 2. 클러스터 간 동기화로 인한 메모리 오버플로
## 2.1 배경
- 듀얼 프로세서에 8GB 메모리를 갖춘 HP 미니 컴퓨터 2대와 웹로직 9.2를 각 세 개씩 구동해 총 6개의 노드로 클러스터를 구성
- 선호도 클러스터이므로 세션 동기화는 없지만 **일부 데이터를 공유하는 상황**
- 기존엔 공유 데이터를 DB로 관리 했지만 읽기/쓰기 작업에 따라 경합이 자주 발생
- JBossCache로 글로벌 캐시를 구축해 해결
- 하지만 간헐적으로 메모리 오버플로 발생

## 2.2 원인
- 간헐적으로 발생하는 것으로 봤을 때, 자주 수행되지 않는 코드에서 메모리 누수가 발생하는 것으로 파악
- JBossCache는 내부적으로 JGroups라는 개념을 이용한다.
- JGroups는 패킷을 송/수신할 때, **불안정한 네트워크 통신에 대비해 클러스터 내의 모든 노드가 패킷을 제대로 수신했다는 확인을 할 때까지 메모리에 패킷을 보관**
- 서버에선 글로벌 필터를 사용하며, 해당 **필터는 동기화 작업 때문에 모든 노드들과 네트워크 통신이 빈번**한 상황
- 이 과정에서 **네트워크 처리량 문제로 패킷이 유실**되고, 제대로 **수신했다는 확인을 하지 못해 패킷을 계속해서 메모리에 보관하게 되며 메모리 오버플로가 발생**

즉, JBossCache의 결함과 시스템 구현 방식이 문제 원인이다. 
클러스터 전체에서 공유해야 하는 데이터를 분산 클러스터 캐시를 이용해 동기화 하는 경우 네트워크 통신이 자주 일어나므로 주의가 필요함

# 3. 힙메모리 부족으로 인한 오버플로 오류
## 3.1 배경
- 서버 푸시 기술을 이용하여 클라이언트가 서버로부터 데이터를 실시간으로 받는 시스템
- 서버 푸시 프레임워크는 CometD 1.1.1, 서버 소프트웨어는 제티 7.1.4
- i5 CPU, 4GB 메모리, 32비트 윈도우를 사용
- 테스트 중 서버에서 메모리 오버플로가 발생하며 여유 메모리가 부족해 힙 덤프 스냅숏도 생성하지 못함
- jstat을 이용해 모니터링 후 가비지 컬렉션이 빈번하지 않음을 파악
- 이후 시스템 로그를 통해 메모리 오버플로 예외를 확인
	- 32비트 윈도우의 경우 개별 프로세스가 할당 받을 수 있는 메모리는 최대 2GB
	- 그 중 1.6GB가 힙에 할당되고 나머지 0.4GB의 일부는 다이렉트 메모리에 할당됨
## 3.2 원인
- 다이렉트 메모리 또한 GC의 대상이지만 전체 GC가 발생할 때만 회수됨
	- 전체 GC는 구세대가 꽉 차는 경우에만 실행되지만 해당 시스템에선 구세대는 안정되어 있음
- CometD는 다이렉트 메모리를 많이 활용하는 NIO 연산을 매우 많이 수행 
물리 메모리 용량이 적은 시스템이나 32비트 애플리케이션에선 자바 힙/메서드 영역 외에 다른 영역들도 가용 메모리에 상당한 비중을 차지하게 된다. 때문에 총 메모리 사용량이 늘어나 개별 프로세스에 허용된 메모리 용량을 초과하게 되면 오버플로가 발생하는 것이다.

# 4. 시스템을 느려지게 하는 외부 명령어
## 4.1 배경
- 프로세서 4개가 장창된 솔라리스10 시스템에 글래스 피시라는 미들웨어를 사용
- 동시성 부하 테스트 수행 시 응답 속도가 급격히 느려짐
## 4.2 원인
- mpstat으로 확인하니 프로세서 이용률은 높았지만 자원 소비 주체가 해당 시스템이 아님을 확인
- 솔라리스10의 dtrace 스크립트를 이용해 fork 시스템 콜이 자원 소비 주체임을 확인
- 시스템 내부에선 요청을 처리할 때 특정 시스템 정보가 필요해 외부 셸 스크립트를 실행하는데 이는 자원을 매우 많이 소비하게 된다. (프로세스 생성 비용)
	- 현재 VM과 같은 환경 변수 설정을 공유하는 프로세스를 복사
	- 새로운 프로세스에 외부 명령을 실행
	- 프로세스 종료
- 이 과정에서 프로세서 자원뿐 아니라 메모리 부담 또한 늘어나게 됨

# 5. 서버 가상 머신 프로세스 비정상 종료
