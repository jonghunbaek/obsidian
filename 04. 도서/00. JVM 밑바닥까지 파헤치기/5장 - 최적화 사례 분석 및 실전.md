# 1. 대용량 메모리 기기 대상 배포 전략
## 1.1 배경
- 하루 페이지 뷰가 15만에 육박
- 성능 개선을 위해 서버의 Scale Up을 수행
	- 32비트 OS -> 64비트 OS
	- 힙 사이즈 1.5GB -> 12GB(전체 메모리는 16GB로)
- 서버의 하드웨어 성능 향상 대비 실제 애플리케이션의 성능은 비슷하거나 오히려 떨어지는 경우가 발생

## 1.2 원인
- 문제의 핵심 원인은 GC
	- JDK 5 핫스파의 기본 설정인 패러렐 컬렉터를 사용(패러렐 스캐빈지+패러렐 올드)
	- 일시 정지 시간보단 처리량에 중점을 둔 컬렉터
	- 12GB의 힙 메모리를 전체 GC하며 최대 14초까지 일시 정지하게 됨
	- 웹 페이지를 직렬화하는 과정에서 메모리에 수많은 거대 객체가 쌓이며 대부분 구세대에 생성됨
	- **즉, 힙 메모리를 너무 크게 잡아 회수/재활용에 너무 오랜 시간이 걸림**

## 1.3 해결
현재 상황(대용량 메모리를 갖춘 하드웨어)에서 배포 방식에 따라 두 가지 방안을 사용할 수 있음
- **VM 인스턴스 하나가 거대한 자바 힙 메모리를 관리**
- **VM 여러 개를 동시에 띄워 놓는 논리적 클러스터 구성**

기존엔 관리자가 첫 번째 방식을 사용한다. 만약 이 방식으로 문제를 해결하려면 셰넌도어 또는 ZGC와 같이 **지연 시간 통제를 목표로하는 GC를 이용**해야 한다. (JDK 5기준엔 해당 GC들이 존재하지 않았음)
패러렐 컬렉터를 사용해 해결하려면 **전체 GC 빈도를 최대한 낮추는 것이 중요**하다. 이 때,  전체 GC 빈도를 제어하려면 구세대가 안정되어야 한다. **즉, 오래 생존하는 객체가 적어야 한다는 뜻이다.**(객체의 생존 범위가 요청/페이지 범위를 넘어서면 안됨)

이외에도 첫 번째 방식으로 배포를 하게 되면 아래와 같은 사항들을 추가적으로 검토해야 한다.
- 힙 메모리의 거대 블록 회수로 인한 긴 시간의 일시 정지
- 64비트 VM의 성능은 일반적으로 동일 버전의 32비트 VM보다 조금씩 느림
- 안정적인 애플리케이션
	- 대규모 단일 애플리케이션에선 힙 덤프 스냅숏을 생성하기도 힘들고 생성해도 분석이 어려움
- 32비트 VM보다 64비트 VM에서 더 많은 메모리를 사용

이러한 요소들을 고려했을 때, 저자는 두 번째 방식의 배포가 적합하다고 판단했다.
물론 이 방식에도 아래와 같은 단점들이 존재한다.
- 클러스터 내 노드들의 전역 자원에 대한 경합
- 커넥션 풀과 같은 자원 풀을 효율적으로 활용하기 어려움
	- 중앙화된 JNDI로 해결할 수 있지만 복잡도/성능 비용 증가
- 클러스터 노드로 32비트 VM을 쓴다면 노드별 메모리는 하드웨어 성능과 관계 없이 32비트 메모리 최대 사용량(4GB)을 넘을 수 없다. 
- 로컬 캐시를 많이 사용한다면 상당량의 메모리가 낭비됨
	- 글로벌 캐시를 사용해 해결

최종적으로 32비트 VM 5대로 논리 클러스터를 구축하고, 각 메모리를 2GB씩 할당하여 총 10GB의 메모리를 활용하도록 했다. 또한 사용자의 응답 속도를 고려해 패러렐 컬렉터에서 CMS로 변경했다. 

# 2. 클러스터 간 동기화로 인한 메모리 오버플로
## 2.1 배경
- 듀얼 프로세서에 8GB 메모리를 갖춘 HP 미니 컴퓨터 2대와 웹로직 9.2를 각 세 개씩 구동해 총 6개의 노드로 클러스터를 구성
- 선호도 클러스터이므로 세션 동기화는 없지만 **일부 데이터를 공유하는 상황**
- 기존엔 공유 데이터를 DB로 관리 했지만 읽기/쓰기 작업에 따라 경합이 자주 발생
- JBossCache로 글로벌 캐시를 구축해 해결
- 하지만 간헐적으로 메모리 오버플로 발생

## 2.2 원인
- 간헐적으로 발생하는 것으로 봤을 때, 자주 수행되지 않는 코드에서 메모리 누수가 발생하는 것으로 파악
- JBossCache는 내부적으로 JGroups라는 개념을 이용한다.
- JGroups는 패킷을 송/수신할 때, **불안정한 네트워크 통신에 대비해 클러스터 내의 모든 노드가 패킷을 제대로 수신했다는 확인을 할 때까지 메모리에 패킷을 보관**
- 서버에선 글로벌 필터를 사용하며, 해당 **필터는 동기화 작업 때문에 모든 노드들과 네트워크 통신이 빈번**한 상황
- 이 과정에서 **네트워크 처리량 문제로 패킷이 유실**되고, 제대로 **수신했다는 확인을 하지 못해 패킷을 계속해서 메모리에 보관하게 되며 메모리 오버플로가 발생**

즉, JBossCache의 결함과 시스템 구현 방식이 문제 원인이다. 
클러스터 전체에서 공유해야 하는 데이터를 분산 클러스터 캐시를 이용해 동기화 하는 경우 네트워크 통신이 자주 일어나므로 주의가 필요함

# 3. 힙메모리 부족으로 인한 오버플로 오류
## 3.1 배경
- 서버 푸시 기술을 이용하여 클라이언트가 서버로부터 데이터를 실시간으로 받는 시스템
- 서버 푸시 프레임워크는 CometD 1.1.1, 서버 소프트웨어는 제티 7.1.4
- i5 CPU, 4GB 메모리, 32비트 윈도우를 사용
- 테스트 중 서버에서 메모리 오버플로가 발생하며 여유 메모리가 부족해 힙 덤프 스냅숏도 생성하지 못함
- jstat을 이용해 모니터링 후 가비지 컬렉션이 빈번하지 않음을 파악
- 이후 시스템 로그를 통해 메모리 오버플로 예외를 확인
	- 32비트 윈도우의 경우 개별 프로세스가 할당 받을 수 있는 메모리는 최대 2GB
	- 그 중 1.6GB가 힙에 할당되고 나머지 0.4GB의 일부는 다이렉트 메모리에 할당됨
## 3.2 원인
- 다이렉트 메모리 또한 GC의 대상이지만 전체 GC가 발생할 때만 회수됨
	- 전체 GC는 구세대가 꽉 차는 경우에만 실행되지만 해당 시스템에선 구세대는 안정되어 있음
- CometD는 다이렉트 메모리를 많이 활용하는 NIO 연산을 매우 많이 수행 
물리 메모리 용량이 적은 시스템이나 32비트 애플리케이션에선 자바 힙/메서드 영역 외에 다른 영역들도 가용 메모리에 상당한 비중을 차지하게 된다. 때문에 총 메모리 사용량이 늘어나 개별 프로세스에 허용된 메모리 용량을 초과하게 되면 오버플로가 발생하는 것이다.

# 4. 시스템을 느려지게 하는 외부 명령어
## 4.1 배경
- 프로세서 4개가 장창된 솔라리스10 시스템에 글래스 피시라는 미들웨어를 사용
- 동시성 부하 테스트 수행 시 응답 속도가 급격히 느려짐
## 4.2 원인
- mpstat으로 확인하니 프로세서 이용률은 높았지만 자원 소비 주체가 해당 시스템이 아님을 확인
- 솔라리스10의 dtrace 스크립트를 이용해 fork 시스템 콜이 자원 소비 주체임을 확인
- 시스템 내부에선 요청을 처리할 때 특정 시스템 정보가 필요해 외부 셸 스크립트를 실행하는데 이는 자원을 매우 많이 소비하게 된다. (프로세스 생성 비용)
	- 현재 VM과 같은 환경 변수 설정을 공유하는 프로세스를 복사
	- 새로운 프로세스에 외부 명령을 실행
	- 프로세스 종료
- 이 과정에서 프로세서 자원뿐 아니라 메모리 부담 또한 늘어나게 됨

# 5. 서버 가상 머신 프로세스 비정상 종료
## 5.1 배경
- 미니 컴퓨터 2대에 총 6개의 노드로 선호도 클러스터를 구성
- 노드의 VM 프로세스가 닫히는 문제가 빈번히 발생
- 두 컴퓨터의 모든 노드에서 프로세스들이 충돌
- 경영 정보 시스템에서 할 일 목록의 상태가 바뀌면 **사무 자동화 포털 시스템이 웹 서비스를 통해 이 정보를 받아와 동기화 하는 방식**
- 동기화 요청을 별도로 확인해보니 최대 3분의 응답 시간이 걸렸고, 대부분 타임아웃이 발생

## 5.2 원인
- 경영 정보 시스템 이용자가 많아 사무 자동화 시스템 속도에 영향을 최대한 받지 않기 위해 비동기 요청을 보냄
- 이 때, 사무 자동화 시스템이 제때 응답을 하지 못해 대기하는 스레드와 소켓 연결이 많아짐
- 때문에 VM의 한계를 넘어서 프로세스가 비정상 종료

## 5.3 해결
- 비동기 호출을 pub/sub방식의 메시지 큐로 변경

# 6. 부적절한 데이터 구조로 인한 메모리 과소비
## 6.1 배경
- 64비트 JVM을 이용하는 백그라운드 RPC 서버
- 메모리는 -Xms4g, -Xmx8g, -Xmn1g로 설정하고, 파뉴+CMS 컬렉터를 사용
- 대부분의 마이너 GC는 30밀리초 이내라 문제 없어 보였음
- 하지만 데이터 분석 시 100만개 이상의 HashMap<Long, Long> 타입의 객체를 생성하고, 때문에 마이너 GC 시 일시 정지 시간이 500밀리초로 증가

## 6.2 원인
- 데이터 분석시 총 800MB 용량의 에덴이 빠르게 채워져 마이너 GC 후에도 신세대 객체 대부분이 생존하게 됨
- 파뉴 컬렉터는 마크-카피 알고리즘을 사용하고, 대부분의 객체가 생존할 경우 복사 과정에서 상당한 시간을 소요하게 됨
- 근본적인 원인은 HashMap<Long, Long> 타입
	- 해당 구조는 공간을 효율적으로 활용하지 못함 - 유효 데이터 비율이 18%
	- 즉, 18%를 제외한 모두 메타 정보를 위한 배보다 배꼽이 더 큰 상호아
 
## 6.3 해결
- 첫 번째 마이너 GC 이후 생존 객체를 바로 구세대로 옮겨 해결할 수도 있음
	- 다만 해당 방식은 죽은 객체 회수를 다음 메이저 GC에 맡기는 것인데, 증상을 완화할 수는 있지만 부작용이 커 근본적인 해결채은 안도미
-  자료 구조 자체를 변경하는 편이 좋을 듯?

# 7. 윈도우 가상 메모리로 인한 긴 일시 정지
## 7.1 배경
- 서드 파티 서비스로부터 15초마다 데이터를 가져오고, 30초의 타임 아웃이 설정됨
- 제품 출시 후 거짓 양성 데이터가 종종 섞여 들어옴
- 프로그램이 약 1분 간격으로 로그 출력 없이 일시 정지 상태가 되기 때문이었음

## 7.2 원인
- VM 매개 변수를 활용해 대부분의 가비지 컬렉션이 100밀리초 이내로 끝나지만 가끔 1분 가까이 걸리는 경우를 발견
- 프로그램 창을 최소화하면 메모리 사용량이 급격하게 줄어들었으나 가상 메모리에는 변화가 없었음(작업 메모리가 디스크로 스왑 된다고 짐작할 수 있음)
- 이 상태에서 가비지 컬렉션을 수행할 경우 스왑된 데이터를 디스크에서 메모리로 다시 불러와야하고 때문에 일시 정지 시간이 길어짐
## 7.3 해결
- 자바 GUI 프로그램에서 이 현상을 없애려면 아래 매개 변수를 추가해야 함
- -Dsun.awt.keepWorkingSetOnMinimize=true

# 8. 안전 지점으로 인한 긴 일시 정지
## 8.1 배경
- 일반적인 컴퓨팅 작업을 처리하는 비교적 큰 HBase 클러스터가 있으며, JDK8과 G1 컬렉터를 이용
- 해당 클러스터는 매일 대량의 맵리듀스, 스파크 오프라인 분석을 수행
- 지연시간이 중요하지 않았기에 GC 최대 일시 정지 시간을 500밀리초로 설정
- 하지만 이후 일시 정지가 3초 이상 걸리는 것을 확인

## 8.2 원인
- 로그를 확인해 실제 GC 작업 시간이 수백 밀리초임을 확인
- 하지만 사용자 스레드가 일시 정지한 시간은 2.26초가 걸림
- 해당 JVM에선 **사용자 스레드가 모두 안전 지점에 도착할 때까지 스레드들이 대기하도록 설정되어 있었음**
- 즉, **몇 개의 스레드가 안전 지점에 도착하지 못해 GC 스레드를 포함한 모든 스레드가 일시 정지하고 있던 상태**
- 안전 지점에 늦게 도착하는 스레드를 확인하니 해당 스레드가 카운티드 루프에서 시간이 오래 걸리는 작업을 수행
- 때문에 안전 지점으로 도착하는 시간이 길어져 전체 일시 정지 시간이 늘어난 것이 주요 원인

>[!NOTE]
>**카운티드 루프란?**
>메서드 호출, 순환문 점프, 비정상적인 점프 등이 안전 지점으로 설정될 수 있다. 하지만 안전 지점이 너무 많아 지는 것을 대비하여 int나 더 작은 데이터 타입을 반복 횟수로 설정할 경우엔 해당 순환문을 안전 지점으로 설정하지 않는다. 대신 **long타입을 반복 횟수로 설정할 경우엔 안전 지점으로 설정되며, 이를 언카운티드 루프**라고 한다.
